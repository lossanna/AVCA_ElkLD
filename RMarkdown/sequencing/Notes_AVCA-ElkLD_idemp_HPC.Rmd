---
title: "Notes for `idemp` on HPC for AVCA-ElkLD project"
author: "Lia Ossanna"
date: "2022-12-19"
output:
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Background
- Demultiplexing forward and reverse reads, received from the [UA Microbiome Core](https://peds.arizona.edu/steele/microbiome-core):
  - `Undetermined_S0_L001_I1_001.fastq.gz`
  - `Undetermined_S0_L001_R1_001.fastq.gz`
  - `Undetermined_S0_L001_R2_001.fastq.gz`
  - List of barcodes for 16S and ITS.
- Example is for 16S, but the process for demultiplexing ITS is the same (just change `16S` to `ITS`).
- [Link to UA HPC guide](https://public.confluence.arizona.edu/display/UAHPC/User+Guide). Used Puma server and submitted job via `SLURM` script.
  
# File organization 
- Files downloaded to Rodent-3 (personal computer).

# 1) Generate reverse-complement barcode list
- This step is done on a personal computer using RStudio.
- I manually edited the barcode lists I received from UA Microbiome Core and created new text files of barcodes because I wanted to change the sample names from what I had provided them.
- Actual script: `AVCA-ElkLD/amplicon-sequencing/demultiplexing/reverse-complement-barcodes.R`.
```{r, eval=FALSE}
library(seqinr)


# 16S_barcodes ------------------------------------------------------------

# Read in a CSV with the barcodes in the first column 
  # and the corresponding sample names in the second column
barcode <- read.table("hpc-amplicon-sequencing/demultiplexing/16S_barcodes.txt", header = TRUE) # change to your file name

# Get reverse complement of barcodes 
barcode$Barcode_revcomp <- sapply(barcode[ , 1], 
                                  function(x) {
                                    toupper(c2s(rev(comp(s2c(x)))))
                                  }
)

# Build dataframe
barcode_revcomp <- data.frame("Barcode" = barcode$Barcode_revcomp, 
                              "SampleID" = barcode[ , 2])

# Write dataframe to table 
write.txt(barcode_revcomp, 
          "hpc-amplicon-sequencing/demultiplexing/16S_barcodes_rev.txt", 
          sep = "\t", 
          quote = FALSE, 
          row.names = FALSE)




# ITS barcodes ------------------------------------------------------------

# Read in a CSV with the barcodes in the first column 
  # and the corresponding sample names in the second column
barcode <- read.csv("hpc-amplicon-sequencing/demultiplexing/ITS_barcodes.csv", header = TRUE) # change to your file name

# Get reverse complement of barcodes 
barcode$Barcode_revcomp <- sapply(barcode[ , 1], 
                                  function(x) {
                                    toupper(c2s(rev(comp(s2c(x)))))
                                  }
)

# Build dataframe
barcode_revcomp <- data.frame("Barcode" = barcode$Barcode_revcomp, 
                              "SampleID" = barcode[ , 2])

# Write dataframe to table 
write.csv(barcode_revcomp, 
            "hpc-amplicon-sequencing/demultiplexing/ITS_barcodes_rev.csv",
          row.names = FALSE)
```

# 2) Create directories in UA HPC
- Log on to UA HPC via <https://ood.hpc.arizona.edu>.
- Created the folders:
  - `/groups/egornish/lossanna/`
  - `/groups/egornish/lossanna/AVCA-ElkLD_16S/`
  - `/groups/egornish/lossanna/AVCA-ElkLD_16S/raw_data/`
  - `/groups/egornish/lossanna/AVCA-ElkLD_16S/scripts/`
  - `/groups/egornish/lossanna/AVCA-ElkLD_ITS/`
  - `/groups/egornish/lossanna/AVCA-ElkLD_ITS/raw_data/`
  - `/groups/egornish/lossanna/AVCA-ElkLD_ITS/scripts/`

# 3) Upload `fastq.gz` files to `raw_data/`
- Transfer `I1.fastq.gz`, `R1.fastq.gz`, and `R2.fastq.gz` files into `raw_data/` directory using a file transfer program (see [here](https://public.confluence.arizona.edu/display/UAHPC/Transferring+Data) for instructions).
  + Used FileZilla via SFTP (Port 22); NetID is username, and password is the one associated with NetID/WebAuth.

# 4) Upload reverse-complement barcode list to `raw_data/`
- Upload the reverse complement barcode `.txt` file generated in Step 1 to the `raw_data/` directory.
  + The file is very small so you can upload directly from Open OnDemand portal 
  (navigate to directory and click Upload in the top right corner).

# 5) Access Puma shell
- Return to the [Open OnDemand dashboard](https://ood.hpc.arizona.edu).
- Open the shell.
  + Click Clusters --> Shell Access.
- The remaining steps will be carried out in the shell. The Puma shell is the default UA HPC cluster (Ocelote and ElGato are both older clusters).

# 6) Install `idemp` (if needed)
- `idemp` was already installed under `/groups/egornish/software/idemp/`.

# 7) Create and write `SLURM` script
- Navigate to `scripts/` folder using the command `cd path`.
  + Example: `cd /groups/PI/netid/project_16S/scripts`.
- Create a file that will become the script used to submit job to the Puma server using the command `touch filename`.
  + Example: `touch 16S_demultiplex.slurm`
- Edit the file by using the command `nano filename` (`nano` is a text editor built into the Puma shell).
  + Example: `nano 16S_demultiplex.slurm`
- Copy and paste the following code into the text editor, changing `16S` to `ITS` where necessary, changing `PI` to appropriate name, and changing paths for files, if they are different than example ones. Then save the file using `Ctrl+O` and then hitting `Enter`, and exit the text editor using `Cntrl+X`.
  + Note: Because this document was made in `RMarkdown`, the first chunk of code looks like comments because they start with `#`. In `Linux` (the Puma shell), these are not comments, but very necessary commands. Comments in this code contain a space: `# comment`.
- Actual `SLURM` scripts were downloaded and saved in `amplicon-sequencing/demultiplexing/`.
```{r, eval=FALSE}
#!/bin/bash

#SBATCH --job-name=16S_demultiplex
#SBATCH --nodes=1
#SBATCH --ntasks=94
#SBATCH --time=00:30:00
#SBATCH --account=gornish
#SBATCH --partition=standard
#SBATCH --mail-type=END
#SBATCH --mail-user=lossanna@arizona.edu

# Set working directory
cd /groups/egornish/lossanna/AVCA-ElkLD_16S/raw_data

# demultiplex using idemp
/groups/egornish/software/idemp/idemp \
-b 16S_barcodes_rev.txt \
-I1 Undetermined_S0_L001_I1_001.fastq.gz \
-R1 Undetermined_S0_L001_R1_001.fastq.gz \
-R2 Undetermined_S0_L001_R2_001.fastq.gz \
-m 1 \
-o 16S_demultiplexing/ # change me to your path
```

# 8) Submit the job 
- Submit the job using the command `sbatch SLURMscript`.
	+ Example: `sbatch 16S_demultiplexed.slurum`.
- You will be provided a job number (all numerical digits). The job number is needed to monitor the status of the job, or view details about the job from the shell/terminal.

# 9) Monitor the job
-	Check the status of the job using the command `squeue jobnumber`
- See [this page](https://public.confluence.arizona.edu/display/UAHPC/Running+Jobs+with+SLURM) of the UA HPC guide for more details. You can also add to the `SLURM` script to have an email sent to you when the job is finished.
-	For reference, 68 samples can be demultiplexed in about 22 minutes using the HPC conditions in this example.


# Breakdown of the `SLURM` script
## First chunk
- `#!/bin/bash` means we are executing the script using the Bash shell.
- The first 7 lines that start with `#SBATCH` specify the parameters of the job (see [this page](https://public.confluence.arizona.edu/display/UAHPC/Running+Jobs+with+SLURM#RunningJobswithSLURM-batch-directivesBatchJobDirectives) of the UA HPC guide for more details).
- `--job-name` is the name of the job.
- `--nodes` is the number of nodes to be used, which relates to the computing power requested; on Puma, one node has 94 cores (a medium-grade laptop usually has about 4 cores).
- `--ntasks` is the number of cores per node requested.
-	`--time` specifies the time limit (the real-time/wall-clock time the job will be allowed to run).
	+ In this example, the job will be terminated after an hour regardless of if it is finished; however, 1 hour will be plenty of time.
-	`--partition` can be `standard` or `windfall`, and refers to priority in assigning jobs, where `windfall` has lower priority (usually you will use `standard`, unless your PI uses the HPC system a lot).
- `--account` is the name of the PI of whom to bill the hours/computing time.
  - Note that for Elise Gornish's group, the group name is "gornish" even though her NetID and the `/groups` folder is "egornish".
- `--mail-type=END` denotes that you will be emailed when the job is finished (`END`; can also be emailed when the job starts with `BEGIN`, or if it fails with `FAIL`).
- `--mail-user` is the email to send the notifications to.
-	These commands are standard for submitting any job to Puma.

## Second chunk
-	The next line after the break is a command (`cd`) to set the working directory.

## Third chunk
- The first line (after the comment) is the path to where the `idemp` file is located that contains the actual demultiplexing code; it is within the `idemp` directory created when `idemp` was installed.
- `-b` is the path to where the barcode labels are located; we have to use the reverse complements.
- `-I1` is the `I1.fastq.gz` file.
-	`-R1` is the `R1.fastq.gz` file.
-	`-R2` is the `R2 fastq.gz` file.
-	`-m` is the number of base mismatches allowed; the default is `1`.
-	`-o` is the output folder where the demultiplexed files will be sent.

